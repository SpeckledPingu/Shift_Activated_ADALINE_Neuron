{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift Activated ADALINE Neuron\n",
    "\n",
    "**The principle behind this activation function is that a misclassification of a target value generates noise in previously established reference predictions. Thus, it will weaken known predictions**\n",
    "\n",
    "***Speed and accuracy are both an improvement for tweet classification over the stock SKLEARN MLPClassifier***\n",
    "\n",
    "The structure for using this neuron is thus:\n",
    "\n",
    "***Establish the reference prediction***\n",
    "\n",
    "neuron.fit(train_set)\n",
    "\n",
    "reference_prediction = neuron.predict(reference_set)\n",
    "\n",
    "***Predict unknown tweet***\n",
    "\n",
    "Classify tweet as belonging to class 0 (call this tweet:0)\n",
    "\n",
    "neuron.fit(train_set + tweet:0)\n",
    "\n",
    "tweet_prediction = neuron.predict(reference_set)\n",
    "\n",
    "difference(tweet_prediction,reference_prediction)\n",
    "\n",
    "***Calculate***\n",
    "\n",
    "if difference > 0:\n",
    "\n",
    "    tweet:0 was correctly a priori classified\n",
    "\n",
    "else:\n",
    "\n",
    "    tweet:0 was incorrectly a priori classified\n",
    "\n",
    "## What is a Reference Prediction?\n",
    "This neuron works off of a baseline prediction of known classes. In the example below, it trains on 500 tweets, but makes a baseline prediction of 500 tweets whose classifications are known. This creates a reference prediction that is used later in classification. Overall, this means that the training set is split into two parts, the ADALINE *train* set and the adaline *reference* set. These sets are disjoint and operate at different stages of the neuron's use.\n",
    "\n",
    "## So this neuron's predict function makes accurate predictions off of 200 tweets?\n",
    "Nope, in fact, the baseline neuron does rather poorly. But, rather than train on a set of 750 tweets whose classifications are known all at the time of training, the training happens on the baseline training set and a reference prediction on 500 tweets is made with the baseline trained neuron. **It does not matter if the reference predictions are correct!** All that is needed is that there are 500 linear activation values whose classification is already known and saved for later use. The prediction comes from measuring the difference between the reference values and the later predicted values when the training set is modified with the tweet in question.\n",
    "\n",
    "## Wait, the training set is modified?\n",
    "Yes, the baseline training set that is used for the reference predictions has the tweet in question added to it. Then the neuron is retrained on this new dataset and a new set of predictions is made on the reference set.\n",
    "\n",
    "## I'm confused, you're not using the predict function to make a prediction about the tweet in question?\n",
    "I am absolutely not using the predict function in the way it is classically used. What makes this work is that the unknown tweet is assigned a value of 0 before retraining and repredicting the reference. Then, if that is correct assignment, then it has a different effect on the reference prediction than if it is incorrectly assigned.\n",
    "\n",
    "## WTF mate? Why does that work?\n",
    "The theory behind this is rather simple. If a tweet is correctly labeled before being added to the training set, then it strengthens the overall predictive abilities of the neuron. So we see a positive shift in the strength of the overall activation values. If it is incorrectly assigned, so instead of belonging to class 0 it's 1, it acts as noise and weakens the overall predictive abilities of the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"ADAptive LInear NEuron Classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    eta: float\n",
    "        learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        passes over the training dataset\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    errors_\n",
    "        number of misclassifications after each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.1, n_iter = 50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Training Data\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: {Array like} shape = [n_samples, n_features]\n",
    "            training vectors,\n",
    "            where n_samples is the number of samples and n_features is the number of features\n",
    "            \n",
    "        y: array-like, shape = [n_samples]\n",
    "            target values\n",
    "        \n",
    "        returns: self : object\n",
    "        \"\"\"\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        self.errors_ = 0\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            output = self.net_input(X)\n",
    "            self.errors_ = (y - output)\n",
    "            \n",
    "            self.w_[1:] += self.eta * X.T.dot(self.errors_)\n",
    "            self.w_[0] = self.eta * self.errors_.sum()\n",
    "            cost = (self.errors_**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        #return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "        output = X.dot(self.w_[1:]) + self.w_[0]\n",
    "        return output\n",
    "    \n",
    "    def activation(self, X):\n",
    "        \"\"\"Calculate the linear activation weights\n",
    "        later to be used in the redshift determination\"\"\"\n",
    "        return self.net_input(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the class label after unite step in addition to raw activation weights\"\"\"\n",
    "        activation_weights = self.activation(X)\n",
    "        return [np.where(activation_weights > 0.0, 1, -1),activation_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_trump = pd.read_csv(\"DonaldTrumpTweets.csv\")\n",
    "df_trump = df_trump.drop(\"Unnamed: 0\", axis=1)\n",
    "df_savage = pd.read_csv(\"AdamSavageTweets.csv\")\n",
    "df_savage = df_savage.drop(\"Unnamed: 0\",axis = 1)\n",
    "\n",
    "df_all = df_trump.append(df_savage)\n",
    "\n",
    "savage_sample = pd.DataFrame(df_savage.sample(2000,random_state=42).text)\n",
    "savage_sample['identity'] = 1\n",
    "trump_sample = pd.DataFrame(df_trump.sample(2000,random_state=42).text)\n",
    "trump_sample['identity'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def munger(data):\n",
    "    for index, row in data.iterrows():\n",
    "        text = row['text']\n",
    "        text = re.sub(\"@\",\"\",text)\n",
    "        text = re.sub(\"#\",\"\",text)\n",
    "        text = re.sub(\"bit\\.ly.*\\s?\",\"\",text)\n",
    "        text = re.sub(\"instagr\\.am.*\\s?\",\"\",text)\n",
    "        text = re.sub(\"https?:.*\\s?\",\"\",text)\n",
    "        text = re.sub(\"t\\.co.*\\s?\",\"\",text)\n",
    "        text = re.sub(\"pic\\.twitter\\.com\\S*\\s?\",\"\",text)\n",
    "        #### set_value is considered the new preferred way of setting values\n",
    "        #### It is also extremely fast when used with iterrows()\n",
    "        data.set_value(index,\"text\",text)\n",
    "   \n",
    "    #return data\n",
    "\n",
    "munger(savage_sample)\n",
    "munger(trump_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_text = savage_sample.append(trump_sample)\n",
    "X_train = savage_sample[:100].append(trump_sample[:100])\n",
    "X_reference = savage_sample[550:800].append(trump_sample[550:800])\n",
    "X_test = savage_sample[850:900].copy(deep=True)\n",
    "\n",
    "# Automatically classify savage samples as belonging to trump\n",
    "# Later, a difference within activation weights will be used to determine if this\n",
    "# Was the correct classification\n",
    "X_test.identity = 0\n",
    "\n",
    "X_test = X_test.append(trump_sample[850:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit(all_text.text)\n",
    "\n",
    "vec_X_train = vectorizer.transform(X_train.text)\n",
    "vec_X_ref = vectorizer.transform(X_reference.text)\n",
    "vec_X_test = vectorizer.transform(X_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 344 ms, sys: 0 ns, total: 344 ms\n",
      "Wall time: 346 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.AdalineGD at 0x7f4a44c3cda0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdalineGD(n_iter=2000,eta=0.001)\n",
    "\n",
    "%time ada.fit(vec_X_train,X_train.identity.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 282 µs\n"
     ]
    }
   ],
   "source": [
    "# Here I establish a baseline prediction with known classifications\n",
    "# This serves as a reference for correct and incorrect classifications\n",
    "%time reference_value = ada.predict(vec_X_ref)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_ada(test_sample):\n",
    "    predictions = []\n",
    "    for index, row in test_sample.iterrows():\n",
    "        modified_train = X_train.append(row)\n",
    "        vec_X_test = vectorizer.transform(modified_train.text)\n",
    "        ada.fit(vec_X_test,modified_train.identity.values)\n",
    "        predictions.append(ada.predict(vec_X_ref)[1])\n",
    "        #print((np.asarray(prediction)[0:250] - np.asarray(reference_value[0:250])).sum() - (np.asarray(prediction)[250:500] - np.asarray(reference_value[250:500])).sum())\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 s, sys: 0 ns, total: 34.3 s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%time results = test_ada(X_test)\n",
    "predicted_results = []\n",
    "for i in range(len(results)):\n",
    "    # If the data generates noise, i.e. it is incorrectly classfied\n",
    "    # Then the noise it generates will result in a weakening of predictions\n",
    "    # This means that the end result will be less than 0\n",
    "    if (np.asarray(results[i])[0:250] - np.asarray(reference_value[0:250])).sum() - (np.asarray(results[i])[250:500] - np.asarray(reference_value[250:500])).sum() > 0:\n",
    "        predicted_results.append(0)\n",
    "    else:\n",
    "        predicted_results.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with SKLearn's MLPClassifier\n",
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Time for SA-ADALINE: ~ 34s\n",
    "### Total Time for MLPClassifier: ~ 1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 38s, sys: 21.6 s, total: 4min\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append X_train and X_reference to use the same amount of information for prediction\n",
    "# As the SA-ADALINE\n",
    "mpl = MLPClassifier()\n",
    "mpl_train = X_train.append(X_reference)\n",
    "vec_mpl_train = vectorizer.transform(mpl_train.text)\n",
    "%time mpl.fit(vec_mpl_train,mpl_train.identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score for SA-ADALINE: .86\n",
    "### Score for MLPClassifier: .77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77000000000000002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = np.ones(50)\n",
    "_ = np.append(_,np.zeros(50))\n",
    "\n",
    "mpl.score(vec_X_test,_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85999999999999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = np.ones(50)\n",
    "_ = np.append(_,np.zeros(50))\n",
    "accuracy_score(_,predicted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
